{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d264aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d469cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wordlists\n",
    "from uk_nl__wordlists import nl23_political_words, uk24_political_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df140bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataframes\n",
    "all_articles = pd.read_parquet('../b__data-collection-with-web-scraping/datasets/news/all_articles_with_id.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423c5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_articles = all_articles[all_articles['country'] == 'NL']\n",
    "uk_articles = all_articles[all_articles['country'] == 'UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "432bac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final dataset with articles from both countries is of shape 94692\n",
      "11915 are from the Netherlands, constituting both NOS (6565) and NU (5350) articles\n",
      "82777 are from the United Kingdom, constituting both BBC (51177) and The Guardian (31600) articles\n"
     ]
    }
   ],
   "source": [
    "print(f'The final dataset with articles from both countries is of shape {all_articles.shape[0]}')\n",
    "print(f\"{nl_articles.shape[0]} are from the Netherlands, constituting both NOS ({nl_articles[nl_articles['outlet'] == 'NOS'].shape[0]}) and NU ({nl_articles[nl_articles['outlet'] == 'NU'].shape[0]}) articles\")\n",
    "print(f\"{uk_articles.shape[0]} are from the United Kingdom, constituting both BBC ({uk_articles[uk_articles['outlet'] == 'BBC'].shape[0]}) and The Guardian ({uk_articles[uk_articles['outlet'] == 'The Guardian'].shape[0]}) articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab5c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NL and UK articles combined constitute a dataset of shape (94692, 11).\n",
      "\n",
      "And contains the following columns and datatypes:\n",
      "country               object\n",
      "outlet                object\n",
      "id                    object\n",
      "url                   object\n",
      "images                object\n",
      "datetime      datetime64[ns]\n",
      "category              object\n",
      "title                 object\n",
      "paragraphs            object\n",
      "alt_txt               object\n",
      "id_unique             object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "For the NL case, that means 11915 articles from NOS (6565) and NU (5350) combined.\n",
      "For the UK case, that means 82777 articles from BBC (51177) and NU (31600) combined.\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data\n",
    "print(f'All NL and UK articles combined constitute a dataset of shape {all_articles.shape}.\\n')\n",
    "print(f'And contains the following columns and datatypes:\\n{all_articles.dtypes}')\n",
    "print('\\n')\n",
    "print(f'For the NL case, that means {nl_articles.shape[0]} articles from NOS ({nl_articles[nl_articles[\"outlet\"]==\"NOS\"].shape[0]}) and NU ({nl_articles[nl_articles[\"outlet\"]==\"NU\"].shape[0]}) combined.')\n",
    "print(f'For the UK case, that means {uk_articles.shape[0]} articles from BBC ({uk_articles[uk_articles[\"outlet\"]==\"BBC\"].shape[0]}) and NU ({uk_articles[uk_articles[\"outlet\"]==\"The Guardian\"].shape[0]}) combined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcaceba",
   "metadata": {},
   "source": [
    "Write method to classify political articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28503c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_test = nl_articles.sample(n=100, random_state=2)\n",
    "uk_test = uk_articles.sample(n=100, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81684d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_occurrences_triple_column_vectorised(dataframe, colnames_of_interest, wordlists):\n",
    "    df = dataframe.copy().reset_index(drop=True)\n",
    "    \n",
    "    for colname in colnames_of_interest:\n",
    "        df[colname] = df[colname].astype(str)\n",
    "        \n",
    "        for identifier, wordlist_options in wordlists.items():\n",
    "            all_matches = []\n",
    "\n",
    "            multistring_patterns = [\n",
    "                (re.compile(rf'\\b{re.escape(word.lower())}\\b'), word) \n",
    "                for word in wordlist_options.get('multistring_matches', [])\n",
    "            ]\n",
    "            substring_list = wordlist_options.get('substring_matches', [])\n",
    "            exact_list = set(wordlist_options.get('exact_matches', []))\n",
    "\n",
    "            for text in df[colname]:\n",
    "                matches_for_row = []\n",
    "\n",
    "                # Multi-strings\n",
    "                lower_text = text.lower()\n",
    "                for pattern, canonical_word in multistring_patterns:\n",
    "                    if pattern.search(lower_text):\n",
    "                        count = len(pattern.findall(lower_text))\n",
    "                        matches_for_row.extend([canonical_word] * count)\n",
    "\n",
    "                # Sub-strings\n",
    "                if substring_list:\n",
    "                    tokens = re.findall(r'\\w+', text)\n",
    "                    for token in tokens:\n",
    "                        if any(sub.lower() in token.lower() for sub in substring_list):\n",
    "                            matches_for_row.append(token)\n",
    "\n",
    "                # Exact-matches (case sensitive)\n",
    "                if exact_list:\n",
    "                    tokens = re.findall(r'\\w+', text)\n",
    "                    for token in tokens:\n",
    "                        if token in exact_list:\n",
    "                            matches_for_row.append(token)\n",
    "\n",
    "                all_matches.append(matches_for_row)\n",
    "\n",
    "            df[f'{identifier}__{colname}'] = all_matches\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf4bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_test_text = list_occurrences_triple_column_vectorised(nl_test, ['title', 'paragraphs', 'alt_txt'], nl23_political_words)\n",
    "nl_test_text.to_excel('nl_test_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f279e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_test_text = list_occurrences_triple_column_vectorised(uk_test, ['title', 'paragraphs', 'alt_txt'], uk24_political_words)\n",
    "uk_test_text.to_excel('uk_test_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572ef2f",
   "metadata": {},
   "source": [
    "After a number of validation checks, perform the text analysis on the full text (takes approximately 14 minutes to complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e26249",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_text_processed = list_occurrences_triple_column_vectorised(nl_articles, ['title', 'paragraphs', 'alt_txt'], nl23_political_words)\n",
    "#uk_text_processed = list_occurrences_triple_column_vectorised(uk_articles, ['title', 'paragraphs', 'alt_txt'], uk24_political_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f4cef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_text_processed.to_parquet('datasets/NL_articles_text_processed.parquet')\n",
    "#uk_text_processed.to_parquet('datasets/UK_articles_text_processed.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39d24f",
   "metadata": {},
   "source": [
    "Continue from file (correction for The Netherlands only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f9f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_articles_text_processed = pd.read_parquet('datasets/NL_articles_text_processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e81b4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_remove_and_map_words(input, search_list, mapping):\n",
    "    '''\n",
    "    Method that 1) splits the input in modified_input and stripped_input, 2) removes the substrings of the stripped_input\n",
    "    from the modified_input, and 3) adds a mapping of the stripped_input to the modified_input.\n",
    "\n",
    "    Example input: ['GroenLinks/PvdA', 'GroenLinks', 'PvdA', 'PvdA', 'Timmermans']\n",
    "    -- this means that by design of the list_occurrences_double_column method, all substrings of the multistring are duplicates --\n",
    "    modified_input = ['GroenLinks', 'PvdA', 'PvdA', 'Timmermans]\n",
    "    stripped_input = ['GroenLinks/PvdA']\n",
    "    returns: ['PvdA', 'Timmermans', 'GL-PvdA']\n",
    "\n",
    "    :input: dataframe cell containing a wordlist (to be applied in lambda statement)\n",
    "    :search_list: list of possible ways to reference a multistring\n",
    "    :mapping: mapping that should replace any of the references from the search_list with a consistent mapping\n",
    "    :return: modified_list where substring duplicates are removed and the references to the multi-string are made consistent\n",
    "    '''\n",
    "\n",
    "    # Split the input in modified_input and stripped_input, where stripped input contains possible references to a multi-string\n",
    "    modified_input = [i for i in input if i not in search_list]\n",
    "    stripped_input = [i for i in input if i in search_list]\n",
    "\n",
    "    # Loop over the multistring references that appeared in the search_list\n",
    "    for word in stripped_input:\n",
    "        # Split the multistring in substrings\n",
    "        subs = re.split(r'[-/]', word)\n",
    "\n",
    "        # Remove these substrings from the modified input, since they are duplicates\n",
    "        for sub in subs:\n",
    "            if sub in modified_input:\n",
    "                modified_input.remove(sub)\n",
    "\n",
    "        # Append a consistent mapping of the multistring\n",
    "        modified_input.append(mapping)\n",
    "    return modified_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e47c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "glpvda_search_list = ['PvdA-GL', 'GL-PvdA', 'GL/PvdA', 'PvdA/GL', 'GroenLinks/PvdA', 'PvdA/GroenLinks', 'GroenLinks-PvdA', 'PvdA-GroenLinks']\n",
    "glpvda_mapping = 'GL-PvdA'\n",
    "\n",
    "for col in ['GL-PvdA__title', 'GL-PvdA__paragraphs', 'GL-PvdA__alt_txt']:\n",
    "    nl_articles_text_processed[col] = nl_articles_text_processed[col].apply(lambda x: conditional_remove_and_map_words(x, glpvda_search_list, glpvda_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b13b6",
   "metadata": {},
   "source": [
    "Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ba323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_articles_text_processed.to_excel('datasets/nl_articles_text_processed.xlsx')\n",
    "nl_articles_text_processed.to_parquet('datasets/nl_articles_text_processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7d350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f94c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60ac5b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'outlet',\n",
       " 'id',\n",
       " 'url',\n",
       " 'images',\n",
       " 'datetime',\n",
       " 'category',\n",
       " 'title',\n",
       " 'paragraphs',\n",
       " 'alt_txt',\n",
       " 'id_unique',\n",
       " 'PVV__title',\n",
       " 'GL-PvdA__title',\n",
       " 'VVD__title',\n",
       " 'NSC__title',\n",
       " 'D66__title',\n",
       " 'BBB__title',\n",
       " 'CDA__title',\n",
       " 'SP__title',\n",
       " 'FVD__title',\n",
       " 'PvdD__title',\n",
       " 'CU__title',\n",
       " 'SGP__title',\n",
       " 'DENK__title',\n",
       " 'Volt__title',\n",
       " 'JA21__title',\n",
       " 'Bij1__title',\n",
       " 'BvNL__title',\n",
       " 'Positions__title',\n",
       " 'Politics__title',\n",
       " 'Issues__title',\n",
       " 'National__title',\n",
       " 'International__title',\n",
       " 'PVV__paragraphs',\n",
       " 'GL-PvdA__paragraphs',\n",
       " 'VVD__paragraphs',\n",
       " 'NSC__paragraphs',\n",
       " 'D66__paragraphs',\n",
       " 'BBB__paragraphs',\n",
       " 'CDA__paragraphs',\n",
       " 'SP__paragraphs',\n",
       " 'FVD__paragraphs',\n",
       " 'PvdD__paragraphs',\n",
       " 'CU__paragraphs',\n",
       " 'SGP__paragraphs',\n",
       " 'DENK__paragraphs',\n",
       " 'Volt__paragraphs',\n",
       " 'JA21__paragraphs',\n",
       " 'Bij1__paragraphs',\n",
       " 'BvNL__paragraphs',\n",
       " 'Positions__paragraphs',\n",
       " 'Politics__paragraphs',\n",
       " 'Issues__paragraphs',\n",
       " 'National__paragraphs',\n",
       " 'International__paragraphs',\n",
       " 'PVV__alt_txt',\n",
       " 'GL-PvdA__alt_txt',\n",
       " 'VVD__alt_txt',\n",
       " 'NSC__alt_txt',\n",
       " 'D66__alt_txt',\n",
       " 'BBB__alt_txt',\n",
       " 'CDA__alt_txt',\n",
       " 'SP__alt_txt',\n",
       " 'FVD__alt_txt',\n",
       " 'PvdD__alt_txt',\n",
       " 'CU__alt_txt',\n",
       " 'SGP__alt_txt',\n",
       " 'DENK__alt_txt',\n",
       " 'Volt__alt_txt',\n",
       " 'JA21__alt_txt',\n",
       " 'Bij1__alt_txt',\n",
       " 'BvNL__alt_txt',\n",
       " 'Positions__alt_txt',\n",
       " 'Politics__alt_txt',\n",
       " 'Issues__alt_txt',\n",
       " 'National__alt_txt',\n",
       " 'International__alt_txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_articles_text_processed.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab8d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eotw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
